# ============================================================
# PRODUCTION ENGINE – Alpha Stack Pro (Persistent DB + Signals)
# ============================================================
# Features:
# 1) SQLite DB (trades, signals, stats) – persistent
# 2) Idempotent execution (skip if already ran today)
# 3) Fetch only NEW data from Binance (incremental)
# 4) Generate signal → store in DB
# 5) Ready for cron job
# ============================================================

import time
import json
import pickle
import requests
import numpy as np
import pandas as pd
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout

import xgboost as xgb

# ============================================================
# SECTION 0: DATABASE SETUP
# ============================================================

DB_PATH = Path("data/alpha_stack.db")
DB_PATH.parent.mkdir(exist_ok=True)

def init_db():
    """Create tables if they don't exist"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    
    # Signals table
    c.execute("""
        CREATE TABLE IF NOT EXISTS signals (
            id INTEGER PRIMARY KEY,
            date TIMESTAMP UNIQUE,
            p_down REAL,
            p_flat REAL,
            p_up REAL,
            signal TEXT,
            confidence REAL,
            regime TEXT,
            extreme_bull INTEGER,
            ev REAL,
            vol_target REAL,
            tail_risk REAL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # Trades table
    c.execute("""
        CREATE TABLE IF NOT EXISTS trades (
            id INTEGER PRIMARY KEY,
            entry_date TIMESTAMP,
            exit_date TIMESTAMP,
            entry_price REAL,
            exit_price REAL,
            position_pct REAL,
            pnl_pct REAL,
            equity REAL,
            confidence REAL,
            forced_exit INTEGER,
            exit_reason TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # Backtest stats
    c.execute("""
        CREATE TABLE IF NOT EXISTS backtest_stats (
            id INTEGER PRIMARY KEY,
            run_date TIMESTAMP UNIQUE,
            total_return REAL,
            win_rate REAL,
            sharpe REAL,
            max_dd REAL,
            num_trades INTEGER,
            final_equity REAL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # Model metadata
    c.execute("""
        CREATE TABLE IF NOT EXISTS model_meta (
            id INTEGER PRIMARY KEY,
            trained_date TIMESTAMP UNIQUE,
            accuracy REAL,
            precision_buy REAL,
            feature_count INTEGER,
            timesteps INTEGER,
            split_idx INTEGER
        )
    """)
    
    conn.commit()
    conn.close()
    print("[DB] ✓ Tables initialized")

def signal_already_today():
    """Check if signal was already generated today (idempotent)"""
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    today = datetime.now().date()
    c.execute("SELECT COUNT(*) FROM signals WHERE DATE(date) = ?", (today,))
    count = c.fetchone()[0]
    conn.close()
    return count > 0

# ============================================================
# SECTION 1: DATA FETCHING (INCREMENTAL)
# ============================================================

def _safe_get(url, params=None, timeout=30, max_retries=6, backoff=1.6):
    last_err = None
    for attempt in range(max_retries):
        try:
            r = requests.get(url, params=params, timeout=timeout)
            if r.status_code == 429:
                time.sleep(backoff ** attempt)
                continue
            r.raise_for_status()
            return r
        except Exception as e:
            last_err = e
            time.sleep(backoff ** attempt)
    raise RuntimeError(f"GET failed after retries. url={url}, err={last_err}")

def fetch_daily_data_coingecko(coin_id='bitcoin', days=1825):
    print(f"[DATA] Fetching {days} days of {coin_id} daily data (CoinGecko)...")
    url = f"https://api.coingecko.com/api/v3/coins/{coin_id}/market_chart"
    params = {'vs_currency': 'usd', 'days': days, 'interval': 'daily'}

    r = _safe_get(url, params=params)
    data = r.json()

    if 'prices' not in data or 'total_volumes' not in data:
        raise RuntimeError(f"CoinGecko unexpected response keys: {list(data.keys())}")

    prices = np.array(data['prices'])
    volumes = np.array(data['total_volumes'])

    df = pd.DataFrame({
        'date': pd.to_datetime(prices[:, 0], unit='ms', utc=True).tz_convert(None),
        'cg_close': prices[:, 1].astype(float),
        'cg_volume': volumes[:, 1].astype(float),
    }).set_index('date').sort_index()

    df = df[~df.index.duplicated(keep='last')]
    print(f"[DATA] ✓ CoinGecko rows: {len(df)}")
    return df[['cg_close', 'cg_volume']]

def fetch_4h_binance_to_daily(symbol='BTCUSDT', days=1825):
    print(f"[DATA] Fetching ~{days} days 4H data (Binance) for {symbol}...")
    url = "https://api.binance.com/api/v3/klines"

    interval = '4h'
    limit = 1000

    end_ms = int(datetime.utcnow().timestamp() * 1000)
    start_ms = end_ms - int(days * 24 * 3600 * 1000)

    all_rows = []
    last_close_time = None

    while start_ms < end_ms:
        params = {'symbol': symbol, 'interval': interval, 'startTime': start_ms, 'limit': limit}
        r = _safe_get(url, params=params, timeout=30, max_retries=8)
        data = r.json()

        if isinstance(data, dict) and data.get("code"):
            raise RuntimeError(f"Binance API error: {data}")

        if not data:
            break

        close_time = data[-1][6]
        if last_close_time is not None and close_time <= last_close_time:
            break

        all_rows.extend(data)
        last_close_time = close_time

        start_ms = close_time + 1

        if len(data) < limit:
            break

        time.sleep(0.15)

    if not all_rows:
        raise RuntimeError("No Binance 4H data returned.")

    df = pd.DataFrame(all_rows, columns=[
        'open_time','open','high','low','close','volume',
        'close_time','quote_volume','trades','taker_buy_base','taker_buy_quote','ignore'
    ])

    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms', utc=True).dt.tz_convert(None)
    df = df.set_index('open_time').sort_index()
    df = df[['open','high','low','close','volume']].astype(float)

    daily = df.resample('D').agg({
        'open': 'first',
        'high': 'max',
        'low': 'min',
        'close': 'last',
        'volume': 'sum',
    })

    ret_4h = df['close'].pct_change()
    daily['ret4h_mean'] = ret_4h.resample('D').mean()
    daily['ret4h_std']  = ret_4h.resample('D').std()
    daily['range4h_mean'] = (df['high'] - df['low']).resample('D').mean()

    daily = daily.dropna()
    daily.columns = [f'bn_{c}' for c in daily.columns]

    print(f"[DATA] ✓ Binance daily rows: {len(daily)}")
    return daily

# ============================================================
# SECTION 2: INDICATORS + REGIME + FEATURES (from Alpha Stack)
# ============================================================

def calculate_rsi(prices: pd.Series, period=14):
    delta = prices.diff()
    gain = delta.where(delta > 0, 0.0).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0.0)).rolling(window=period).mean()
    rs = gain / (loss + 1e-8)
    return 100.0 - (100.0 / (1.0 + rs))

def calculate_macd(prices: pd.Series, fast=12, slow=26, signal=9):
    ema_fast = prices.ewm(span=fast, adjust=False).mean()
    ema_slow = prices.ewm(span=slow, adjust=False).mean()
    macd = ema_fast - ema_slow
    macd_signal = macd.ewm(span=signal, adjust=False).mean()
    macd_hist = macd - macd_signal
    return macd, macd_signal, macd_hist

def detect_extreme_bull(df: pd.DataFrame):
    df = df.copy()
    ret = df['close'].pct_change()

    df['vol_30d'] = ret.rolling(30).std()
    df['vol_thr'] = df['vol_30d'].rolling(365, min_periods=365).quantile(0.75).shift(1)

    ema50 = df['close'].ewm(span=50, adjust=False).mean()
    ema200 = df['close'].ewm(span=200, adjust=False).mean()
    df['trend_strength'] = (ema50 - ema200) / ema200

    mu = df['trend_strength'].rolling(365, min_periods=365).mean().shift(1)
    sd = df['trend_strength'].rolling(365, min_periods=365).std().shift(1)
    df['trend_z'] = (df['trend_strength'] - mu) / (sd + 1e-8)

    vol_mu = df['volume'].rolling(30, min_periods=30).mean().shift(1)
    vol_sd = df['volume'].rolling(30, min_periods=30).std().shift(1)
    df['volume_z'] = (df['volume'] - vol_mu) / (vol_sd + 1e-8)

    df['is_extreme_bull'] = (
        (df['vol_30d'] > df['vol_thr']) &
        (df['trend_z'] > 1.0) &
        (df['volume_z'] > 1.0)
    ).astype(int)

    return df

def engineer_features(df: pd.DataFrame):
    df = df.copy()

    df['ret_1d']  = df['close'].pct_change()
    df['ret_7d']  = df['close'].pct_change(7)
    df['ret_30d'] = df['close'].pct_change(30)

    for lag in range(1, 31):
        df[f'ret_1d_lag{lag}'] = df['ret_1d'].shift(lag)
    for lag in range(1, 11):
        df[f'ret_7d_lag{lag}'] = df['ret_7d'].shift(lag)

    df['volatility_7d']  = df['ret_1d'].rolling(7).std()
    df['volatility_30d'] = df['ret_1d'].rolling(30).std()

    df['rsi'] = calculate_rsi(df['close'], 14)
    df['rsi_norm'] = df['rsi'] / 100.0

    macd, macd_sig, macd_hist = calculate_macd(df['close'])
    df['macd'] = macd
    df['macd_signal'] = macd_sig
    df['macd_hist'] = macd_hist

    bb_mid = df['close'].rolling(20).mean()
    bb_std = df['close'].rolling(20).std()
    df['bb_width'] = 2.0 * bb_std / (bb_mid + 1e-8)

    for col in ['ret_1d_lag1', 'volatility_7d', 'rsi_norm']:
        mu = df[col].rolling(20).mean()
        sd = df[col].rolling(20).std()
        df[f'{col}_zscore_20'] = (df[col] - mu) / (sd + 1e-8)

    df['rsi_x_vol'] = df['rsi_norm'] * df['volatility_7d']
    df['momentum_x_extreme'] = df['ret_7d_lag1'] * (df['is_extreme_bull'] * 2 - 1)

    df = df.dropna()
    return df

# ============================================================
# SECTION 3: QUANT DECISION ENGINE (from Alpha Stack)
# ============================================================

def vol_target(vol_daily, target_annual=0.12, max_leverage=1.3):
    if vol_daily is None or np.isnan(vol_daily) or vol_daily <= 0:
        return 0.0
    return float(min(max_leverage, (target_annual / np.sqrt(252)) / vol_daily))

def expected_value(p_up, p_down, avg_up, avg_down, cost=0.0015):
    return float(p_up * avg_up - p_down * avg_down - cost)

def uncertainty_gate(p_models, max_std_up=0.08):
    P = np.stack(p_models, axis=0)
    std_up = float(P[:, 2].std())
    return bool(std_up <= max_std_up)

def cvar_1d(returns: pd.Series, alpha=0.05):
    r = returns.dropna().values
    if len(r) < 50:
        return np.nan
    var = np.quantile(r, alpha)
    return float(r[r <= var].mean())

def quant_decision(p_ens, p_models, df_row, returns_hist, avg_up, avg_down):
    p_down, p_flat, p_up = p_ens
    extreme = bool(df_row['is_extreme_bull'] == 1)

    if not uncertainty_gate(p_models):
        return {'signal': 'HOLD', 'reason': 'UNCERTAINTY_GATE', 'position_pct': 0.0}

    ev = expected_value(p_up, p_down, avg_up=avg_up, avg_down=avg_down, cost=0.0015)
    if ev <= 0:
        return {'signal': 'HOLD', 'reason': 'NEGATIVE_EV', 'position_pct': 0.0, 'expected_value': ev}

    buy_thr  = 0.66 if extreme else 0.70
    sell_thr = 0.80 if extreme else 0.70

    if p_up >= buy_thr:
        signal, conf = 'BUY', float(p_up)
    elif p_down >= sell_thr:
        signal, conf = 'SELL', float(p_down)
    else:
        return {'signal': 'HOLD', 'reason': 'BELOW_THRESHOLD', 'position_pct': 0.0, 'expected_value': ev}

    vt = vol_target(float(df_row['vol_30d']))
    risk_cap = 0.02 if extreme else 0.01
    pos_pct = min(risk_cap * vt * (1.15 if extreme else 1.0), risk_cap)

    tail = cvar_1d(returns_hist, alpha=0.05)
    if not np.isnan(tail) and tail < -0.07:
        pos_pct *= 0.6

    regime = 'BULL' if float(df_row['trend_strength']) > 0 else 'BEAR'
    return {
        'signal': signal,
        'confidence': conf,
        'position_pct': float(pos_pct),
        'expected_value': float(ev),
        'vol_target': float(vt),
        'tail_risk': float(tail) if not np.isnan(tail) else None,
        'extreme_bull': extreme,
        'regime': regime
    }

# ============================================================
# SECTION 4: ENSEMBLE PREDICT (from saved models)
# ============================================================

def ensemble_predict(lstm, xgb_m, lr, lr_scaler, X_seq, X_tab):
    p_lstm = lstm.predict(X_seq, verbose=0)
    p_xgb = xgb_m.predict_proba(X_tab)
    p_lr  = lr.predict_proba(lr_scaler.transform(X_tab))
    return 0.4*p_lstm + 0.4*p_xgb + 0.2*p_lr

# ============================================================
# SECTION 5: DAILY JOB (MAIN EXECUTION)
# ============================================================

def run_daily_pipeline():
    """Generate signal for today and store in DB"""
    
    print(f"\n[JOB] Starting daily pipeline at {datetime.now()}")
    
    # Check if already ran today (idempotent)
    if signal_already_today():
        print("[JOB] ⚠️  Signal already generated today. Skipping.")
        return
    
    # Initialize DB
    init_db()
    
    # Load models + scaler
    try:
        lstm = load_model("models/lstm_model.h5")
        with open("models/xgb_model.pkl", "rb") as f:
            xgb_m = pickle.load(f)
        with open("models/lr_model.pkl", "rb") as f:
            lr = pickle.load(f)
        with open("models/lr_scaler.pkl", "rb") as f:
            lr_scaler = pickle.load(f)
        with open("models/feat_scaler.pkl", "rb") as f:
            feat_scaler = pickle.load(f)
        
        with open("models/artifacts_meta.json", "r") as f:
            meta = json.load(f)
        
        feature_cols = meta['feature_cols']
        timesteps = meta['timesteps']
        avg_up = meta['avg_up']
        avg_down = meta['avg_down']
        
        print("[JOB] ✓ Models loaded")
    except Exception as e:
        print(f"[JOB] ❌ Failed to load models: {e}")
        print("[JOB] Run backtest first to train models!")
        return
    
    # Fetch latest data
    df_cg = fetch_daily_data_coingecko('bitcoin', days=100)
    df_bn = fetch_4h_binance_to_daily('BTCUSDT', days=100)
    
    df = pd.merge(df_bn, df_cg, left_index=True, right_index=True, how='inner').sort_index()
    
    df['open'] = df['bn_open']
    df['high'] = df['bn_high']
    df['low']  = df['bn_low']
    df['close']= df['bn_close']
    df['volume']=df['bn_volume']
    
    # Regime + Features
    df = detect_extreme_bull(df)
    df = engineer_features(df)
    
    # Latest row
    latest_row = df.iloc[-1]
    latest_date = df.index[-1]
    
    # Build features
    X_raw = df[feature_cols].iloc[-1:].values
    X_tab = feat_scaler.transform(X_raw)
    
    X_seq_raw = df[feature_cols].iloc[-timesteps:].values.reshape(1, timesteps, -1)
    X_seq_2d = X_seq_raw.reshape(-1, X_seq_raw.shape[2])
    X_seq_scaled = feat_scaler.transform(X_seq_2d).reshape(1, timesteps, -1).astype(np.float32)
    
    # Ensemble prediction
    p_ens = ensemble_predict(lstm, xgb_m, lr, lr_scaler, X_seq_scaled, X_tab)[0]
    p_lstm = lstm.predict(X_seq_scaled, verbose=0)[0]
    p_xgb  = xgb_m.predict_proba(X_tab)[0]
    p_lr   = lr.predict_proba(lr_scaler.transform(X_tab))[0]
    p_models = [p_lstm, p_xgb, p_lr]
    
    # Decision
    returns_hist = df['close'].pct_change().iloc[-365:]
    decision = quant_decision(p_ens, p_models, latest_row, returns_hist, avg_up=avg_up, avg_down=avg_down)
    
    # Store signal in DB
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    
    c.execute("""
        INSERT OR REPLACE INTO signals 
        (date, p_down, p_flat, p_up, signal, confidence, regime, extreme_bull, ev, vol_target, tail_risk)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """, (
        latest_date,
        float(p_ens[0]),
        float(p_ens[1]),
        float(p_ens[2]),
        decision['signal'],
        decision.get('confidence', np.nan),
        decision.get('regime', 'UNKNOWN'),
        int(decision.get('extreme_bull', 0)),
        decision.get('expected_value', np.nan),
        decision.get('vol_target', np.nan),
        decision.get('tail_risk', np.nan)
    ))
    
    conn.commit()
    conn.close()
    
    print(f"[JOB] ✓ Signal: {decision['signal']} (P_up={p_ens[2]:.2%}, P_down={p_ens[0]:.2%})")
    print(f"[JOB] ✓ Regime: {decision.get('regime')} | Extreme Bull: {decision.get('extreme_bull')}")
    print(f"[JOB] ✓ Stored in DB at {latest_date}")
    print(f"[JOB] Completed successfully\n")

if __name__ == '__main__':
    run_daily_pipeline()
